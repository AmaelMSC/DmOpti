{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aa35a47-edc2-499e-89fe-ad1834382f18",
   "metadata": {},
   "source": [
    "Nom, prénom :\n",
    "\n",
    "Nom, prénom :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a649b79a-2ee8-47ae-91ca-f733625b685c",
   "metadata": {},
   "source": [
    "# Descente du gradient avec pas cycliques\n",
    "\n",
    "Dans ce devoir on va introduire une amelioration de la methode de la descente du gradient qui permet de trouver plus efficacement le minimiseur d'une fonction quadratique.\n",
    "\n",
    "L'idée principale de cette méthode et de ne pas se limiter à des itérations de descente avec un pas constant, mais plutôt de considérer une liste de $K$ pas à utiliser de manière cyclique le long les itérations. Ce dégré de liberté en plus permet de construire une famille de méthodes qui ont la même complexité que l'algorithme à pas constant mais qui converge plus vite vers le minimiseur de la fonction quadratique considère. Pour mieux comprendre cette méthode, on va commencer par réintroudire la méthode de la descente du gradient classique.\n",
    "\n",
    "**Nota bene :** certaines questions demandent des responses sous forme de dissertation (démonstration, calcul, discussion), vous pouvez soit donner la reponse dans la cellule de texte après la question, soit rajouter dans cette cellule une image de votre reponse manuscrite. N'oubliez pas d'envoyer tous vos fichiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a2f876-7cd4-4743-99dc-d116207a710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les librairies \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac899fb5-de31-4ea9-8235-39b3e7269652",
   "metadata": {},
   "source": [
    "## I. Rappel : la descente du gradient pour résoudre un système d'équations linéaires\n",
    "\n",
    "Soit $A \\in \\mathcal{M}_n(\\mathbb{R})$ une matrice réelle, symétrique et définie positive, et $b \\in \\mathbb{R}^n$ un vecteur. On note $0 < \\mu  \\leq L$ la plus petite et la plus grande valeur propre de $A$ ($\\lambda_{min} = \\mu$ et $\\lambda_{max} = L$), de telle façon que le spectre de la matrice $A$ vérifie\n",
    "$$\n",
    "    \\mathrm{Spec}(A) \\subset [\\mu, L].\n",
    "$$\n",
    "Résoudre le système \n",
    "$$\n",
    "Ax = b\n",
    "$$\n",
    "est alors équivant à trouver le minimiseur de la fonction $f : \\mathbb{R}^n \\to \\mathbb{R}$ définie par\n",
    "$$\n",
    "    f(x) = \\frac12 \\langle Ax, x \\rangle - \\langle b, x \\rangle.\n",
    "$$\n",
    "Dans le cours, vous avez introduit l'algorithme de la descente du gradient qui permet de construire une suite de vecteurs $(x_k)_{k\\in\\mathbb{N}}$ approchant le minimiseur de $f$ : pour $h > 0$ donné, on considère les itérations\n",
    "$$\n",
    "\\begin{cases}\n",
    "x_0 \\in \\mathbb{R}^n \\\\\n",
    "x_{k+1} = x_k - h \\nabla f(x_k) = (I-hA)x_{k} + h b\n",
    "\\end{cases}\\tag{GradDesc}\n",
    "$$\n",
    "On rappelle ici l'analyse de convergence de cette méthode dans cas quadratique - loi d'iterations linéaire (voir TD4 - Exercices 6 et 7 pour les détails). Soit le vecteur $x^* = A^{-1} b$, minimiseur de $f$, et posons la suite $(e_k)_k \\subset \\mathbb{R}^n$ des erreurs\n",
    "$$\n",
    "e_{k} = x_k - x^*.\n",
    "$$\n",
    "On a alors\n",
    "$$\n",
    "\\begin{cases}\n",
    "e_0 = x_0 - x^*  \\\\\n",
    "e_{k+1} = (I-hA)e_{k} \n",
    "\\end{cases} \\implies e_{k} = (I-hA)^k e_0\n",
    "$$\n",
    "et\n",
    "$$\n",
    "\\|e_k\\| \\leq \\|I-hA\\|^k \\|e_0\\|.\n",
    "$$\n",
    "\n",
    "\n",
    "**Question 0 :** Montrez que pour toute matrice symétrique $B \\in \\mathcal{M}_n(\\mathbb{R})$, la norme subordonnée à la norme éuclidienne vérifie\n",
    "$$\n",
    "    \\|B\\| = \\rho(B) = \\max\\{ |\\lambda| : \\lambda \\in \\mathrm{Spec}(B)\\}\n",
    "$$\n",
    "et qu'alors, on a l'estimation suivante de l'erreur de convergence pour la méthode $\\text{(GradDesc)}$\n",
    "$$\n",
    "    \\|e_k\\| \\leq \\rho(I-hA)^k \\|e_0\\|.\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519449eb-2380-463e-a333-e2e005a4aa9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3052ab-1440-499a-a277-0963c101c1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1b7590f-a9f4-4e70-bb01-ca0b7ff42195",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dffd3cb-e52b-4828-982e-4021b33e2401",
   "metadata": {},
   "source": [
    "On obtient alors la condition (nécessaire et) suffisante pour la convergence de l'algorithme\n",
    "$$\n",
    "    \\text{$\\forall x\n",
    "_0 \\in \\mathbb{R}^n$, (GradDesc) converge} \\iff \\rho(I-hA) < 1 \\iff 0 < h < \\frac{2}{L}.\n",
    "$$ \n",
    "De plus, le pas optimal garantissant la meilleure vitesse de convergence est obtenu en minimisant le rayon spectral de la matrice $I - hA$, c'est-à-dire en résolvant le problème suivant\n",
    "$$\n",
    "    h_{\\mathrm{opt}} = \\underset{h > 0}{\\operatorname{argmin}} \\rho(I-hA).\n",
    "$$    \n",
    "Or, pour toute matrice symétrique $A \\in \\mathcal{M}_n(\\mathbb{R})$, le rayon spectral de la matrice $I-hA$ est obtenu par le\n",
    "$$\n",
    "    \\rho(I-hA) = \\operatorname{max} \\{|1-h\\lambda|, \\lambda \\in \\mathrm{spe}(A)\\}\n",
    "$$\n",
    "d'où\n",
    "$$\n",
    "    h_{\\mathrm{opt}} = \\underset{h > 0}{\\operatorname{argmin}} \\rho(I-hA) = \\underset{h > 0}{\\operatorname{argmin}} \\operatorname{max}  \\{|1-h\\lambda|, \\lambda \\in \\mathrm{spe}(A)\\} =  \\underset{h > 0}{\\operatorname{argmin}}  \\operatorname{max} \\{|1-h\\mu|, |1-hL|\\} = \\frac{2}{\\mu + L}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d3239f-b84b-4a6b-9f66-5ce8c8593a0c",
   "metadata": {},
   "source": [
    "**Question 1 :** Tracez sur un même graphique, les courbes $h \\mapsto |1-h\\mu|$, $h \\mapsto |1-hL|$ et $h \\mapsto |1-h\\lambda|$, pour trois $\\lambda \\in ]\\mu , L[$, avec $\\mu = 1$ et $L = 2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e259dbae-ddae-4399-b796-567b8260be37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f579df45-b0f3-4394-b2af-37cad34901c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cb5cbfc-04e3-42b7-8a77-2dc8ef3e0dae",
   "metadata": {},
   "source": [
    "**Question 2 :** Écrivez la fonction `GradDesc(A,b, x0, h, k) ` qui applique l'algorithme du gradient à pas fixe à $f$, en partant de `x0` avec un pas de temps `h` qui\n",
    "- s'arrête après $k$ itérations,\n",
    "- renvoie la suite des itérés $(x_k)_k$ générée par l'algorithme $\\text{(GradDesc)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8b9172-f6db-4a40-990b-9a2a349164d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5243c2af-ed5e-449f-9316-f19ba62864be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "693a5a5a-2725-4f95-8778-741fa068b6ab",
   "metadata": {},
   "source": [
    "**Question 3 :** Pour une matrice symétrique définie positive $A \\in \\mathcal{M}_{10} (\\mathbb{R})$ et un vecteur $b \\in \\mathbb{R}^{10}$ générés aléatoirement, tracez la courbe d'erreur $(\\| x_k - x^* \\|)_k$ en fonction de $k \\in [0,100]$.\n",
    "\n",
    "*Astruce : pour générer une matrice symétrique définie positive, on pourra générer aléatoirement une matrice $B$ puis considèrer $A = B^TB$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca157130-8005-4957-9ef4-8bc8b4a6d305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e211b030-8b5f-4b8f-b187-147a39e9eea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7126e94-22f8-426f-ba09-5f2cb91350a2",
   "metadata": {},
   "source": [
    "### Une autre interprétation du pas optimal\n",
    "On presente maintenant une autre interpretation du problème pour obtenir le pas optimal. Cette interprétation se prête mieux à l'introduire de la méthode à pas cyclique. On peut observer que :\n",
    "- premierement, l'algorithme se base sur la connaissance de la localisation du spectre de la matrice $A$, et notamment on a supposé que\n",
    "$$\n",
    "    \\mathrm{Spec}(A) \\subset [\\mu, L].\n",
    "$$\n",
    "- deuxièmement, pour choisir le pas optimal on a résolu le problème\n",
    "$$\n",
    "    h_{\\mathrm{opt}} = \\underset{h > 0}{\\operatorname{argmin}} \\underset{\\lambda \\in [\\mu,L]}{\\operatorname{max}} |1-h\\lambda|.\n",
    "$$\n",
    "\n",
    "C'est-à-dire, parmi tous les polynômes de degré $1$ qui valent $1$ en $0$ (de la forme $1-h\\lambda$), on cherche celui dont le maximum de la valeur abolue dans $[\\mu,L]$ est la plus petite : $h$ est alors la reciproque de la racine de ce polynôme. On voit alors sur le graphe suivant que le polynôme $P_{opt} \\, : \\, \\lambda \\mapsto |1-h_{opt} \\lambda|$, avec $h_{opt} = \\frac{2}{L+\\mu}$ satisfait bien cette propriété :\n",
    "\n",
    " ![](min_poly.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4943bc-4b36-4578-b2e1-cfcee8d37393",
   "metadata": {},
   "source": [
    "## II. La descente du gradient 2-cyclique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6806b1-d188-47e7-b49e-0b5d6bb77559",
   "metadata": {},
   "source": [
    "On présente maintenant une première généralisation de la méthode de la descente du gradient. On considère l'algorithme usuel mais, plutôt que choisir un pas $h$ constant, on se donne la possibilité d'alterner entre deux pas differents. C'est-à-dire, pour $h_1, h_2 > 0$, on définit l'algorithme suivant\n",
    "$$\n",
    "\\begin{cases}\n",
    "x_0 \\in \\mathbb{R}^n \\\\\n",
    "x_{k+1} = x_k - h_1 \\nabla f(x_k) &\\text{si $k$ paire} \\\\\n",
    "x_{k+1} = x_k - h_2 \\nabla f(x_k) &\\text{si $k$ impaire}\n",
    "\\end{cases}\\tag{GradDesc 2-cyc}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6bb447-10d3-408b-a14e-3ce394986319",
   "metadata": {},
   "source": [
    "**Question 4 :** Répétez l'analyse de convergence, rappelée dans la section I, pour montrer que\n",
    "$$\n",
    "    e_{2k} = \\left[(I-h_2 A)(I-h_1A)\\right]^k e_0\n",
    "$$\n",
    "et, en particulier, que\n",
    "$$\n",
    "    \\text{(GradDesc 2-cyc) converge} \\iff \\rho((I-h_2A)(I-h_1A)) < 1.\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605327b5-a513-4ec8-a3c1-8b98dd31c6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02682e8-63fa-40db-a60b-cca14c574254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "541cc9da-249b-496c-a23e-f30447c37455",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa57a32-7e49-4d5a-b2aa-bcb41a0cc3a8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ea551e-d94f-42d0-8aa7-a33e1fb57da8",
   "metadata": {},
   "source": [
    "En copiant l'analyse précédente, on a\n",
    "$$\n",
    "\\rho((I-h_2A)(I-h_1A)) \\leq \\underset{\\lambda \\in [\\mu,L]}{\\operatorname{max}} |(1-h_2\\lambda)(1-h_1\\lambda)|,\n",
    "$$\n",
    "et alors on cherche un couple $(h_{1,\\mathrm{opt}}, h_{2,\\mathrm{opt}})$ tel que \n",
    "$$\n",
    "    (h_{1,\\mathrm{opt}}, h_{2,\\mathrm{opt}}) = \\underset{h_1, h_2 > 0}{\\operatorname{argmin}} \\underset{\\lambda \\in [\\mu,L]}{\\operatorname{max}} |(1-h_2\\lambda)(1-h_1\\lambda)|.\n",
    "$$\n",
    "\n",
    "On peut alors montrer (voir aussi le graphique ci-dessous) que la fonction $(h_1 , h_2) \\mapsto  \\underset{\\lambda \\in [\\mu,L]}{\\operatorname{max}} |(1-h_2\\lambda)(1-h_1\\lambda)|$ est minimale pour \n",
    "$$\n",
    "    h_{1,\\mathrm{opt}} = \\frac{2}{L+\\mu} \\frac{1}{1 + \\frac{L-\\mu}{L+\\mu} \\cos\\left(\\frac{\\pi}{4}\\right)}, \\quad h_{2,\\mathrm{opt}} = \\frac{2}{L+\\mu} \\frac{1}{1 +\\frac{L-\\mu}{L+\\mu} \\cos\\left(\\frac{3\\pi}{4}\\right)}\n",
    "$$\n",
    " ![](min_poly2.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20778dfb-bd34-4e3a-9c70-add7b770e5b6",
   "metadata": {},
   "source": [
    "**Question 5 :** Pour une matrice symétrique définie positive $A \\in \\mathcal{M}_{10} (\\mathbb{R})$ générée aléatoirement, comparez les fonctions $\\lambda \\in [\\mu,L] \\mapsto |I-h_{\\mathrm{opt}}\\lambda|^2$ et $\\lambda \\in [\\mu,L]  \\mapsto |(1-h_{1,\\mathrm{opt}} \\lambda)(1-h_{2,\\mathrm{opt}} \\lambda)|$, afin de vérifier graphiquement si on a l'amelioration souhaitée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8b6829-d60e-45c7-b432-59ac700dd417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b7960a-630e-413c-abde-7154f5e4138d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aca9ce0b-e7f8-40cb-994e-7fdc7f76f412",
   "metadata": {},
   "source": [
    "On peut observer que le polynôme avec racines distinctes a effectivement une valeur absolue maximale inferieure, mais on constate aussi que le deuxième polynôme n'est pas toujours dominé par le premier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c259d6-00da-406e-9fc2-7221839a1f3d",
   "metadata": {},
   "source": [
    "**Question 6 :** Écrivez la fonction `GradDesc2(A,b, x0, h, k) ` qui applique l'algorithme du gradient 2-cyclique à $f$, en partant de `x0` avec un vecteur de pas de temps `h`$\\in\\mathbb{R}^2$ qui\n",
    "- s'arrête après $k$ itérations,\n",
    "- renvoie la suite des itérés $(x_k)_k$ générée par l'aglorithme $\\text{GradDesc 2-cyc}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c48ee1-ec96-4d14-bab1-e06e745abf61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17639bd-ab5c-40a9-bceb-d717fcf392ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80566a3a-db18-4cc8-b3f3-459ab4c9c01b",
   "metadata": {},
   "source": [
    "**Question 7 :**  Pour une matrice symétrique définie positive $A \\in \\mathcal{M}_{10} (\\mathbb{R})$ et un vecteur $b \\in \\mathbb{R}^{10}$ générés aléatoirement, tracez la courbe d'erreur $(\\| x_k - x^* \\|)_k$ en fonction de $k \\in [0,100]$, où $(x_k)_k$ a été générée avec l'algorithme $\\text{(GradDesc 2-cyc)}$. Comparez-la avec la courbe d'erreur obtenue avec la méthode du gradient classique $\\text{(GradDesc)}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4c5391-8178-44ed-a84d-59ab0ff629e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3635625b-e7ca-4820-8d01-12ba5f736a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee35285c-bc42-419a-84e8-d5b0db6693c3",
   "metadata": {},
   "source": [
    "**Question 8 :** On peut remarquer que la courbe de l'erreur pour la méthode 2-cyclique a une allure moins lisse, plus en forme de zigzag que la courbe pour la méthode de descente classique. Comment expliquez vous ce comportement ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faee3c9-acd2-4ae3-9f1c-674025bdd7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fea34f5-6319-4555-9c5b-54d37aa5e4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9fc3686-b86d-4e2b-845a-446edd92c28f",
   "metadata": {},
   "source": [
    "## III. La descente du gradient K-cyclique\n",
    "\n",
    "On peut généraliser l'analyse précédente au cas d'un cycle à $K$ pas. L'algorithme de descente s'écrit\n",
    "$$\n",
    "\\begin{cases}\n",
    "x_0 \\in \\mathbb{R}^n \\\\\n",
    "x_{Kk+1} = x_{Kk} - h_1 \\nabla f(x_{Kk}) \\\\\n",
    "x_{Kk+2} = x_{Kk+1} - h_2 \\nabla f(x_{Kk+1}) \\\\\n",
    "\\dots \\\\\n",
    "x_{Kk+K} = x_{Kk+K-1} - h_K \\nabla f(x_{Kk+K-1}) \\\\\n",
    "\\end{cases}\\tag{GradDesc K-cyc}\n",
    "$$\n",
    "pour le choix de cycle de pas $(h_1, \\dots, h_K)$. On obtient l'estimation de l'erreur de convergence à la fin d'un cycle \n",
    "$$\n",
    "    e_{Kk} = \\left[\\prod_{i=1}^K(I-h_i A)\\right]^k e_0\n",
    "$$\n",
    "et le choix optimal des pas est donné par le problème de minimisation suivant\n",
    "$$\n",
    "    h_{\\mathrm{opt}} = \\underset{h \\in\\mathbb{R}^K_+}{\\operatorname{argmin}} \\underset{\\lambda \\in [\\mu,L]}{\\operatorname{max}} \\left|\\prod_{i=1}^K(1-h_i \\lambda)\\right|.\n",
    "$$\n",
    "On peut réintepreter ce problème comme\n",
    "$$\n",
    "\\min_{p \\in \\mathbb{P}_K(\\mathbb{R})} \\max_{\\lambda \\in [\\mu,L]} |p(\\lambda)|\n",
    "$$\n",
    "où $\\mathbb{P}_K$ est l'ensemble des polynômes (scindés) de dégré $K$ et tels que $p(0) = 1$, et les pas $(h_1, \\dots, h_K)$ seront, à nouveau, les réciproques des zéros du polynôme optimal.\n",
    "\n",
    "Est-il possible de resoudre explicitement ce problème auxilliaire ? De façon un peu étonnante, oui ce problème a une solution explicite et simple à calculer : **les polynômes de Tchebychev** !\n",
    "\n",
    "En étudiant les racines de ces polynômes particuliers, il est possible de montrer que le cycle optimal pour le $\\text{(GradDesc K-cyc)}$ est donné par :\n",
    "$$\n",
    "    h_{i,\\mathrm{opt}} = \\frac{2}{L+\\mu + (L-\\mu) \\cos\\left(\\frac{\\pi (2i-1)}{2K}\\right)}, \\qquad i = 1, \\dots, K.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1f77c2-0f71-4fa3-9305-0751627066e3",
   "metadata": {},
   "source": [
    "**Question 9 :** Pour $K = 3$ et $K = 4$, tracez le polynôme $\\lambda \\mapsto \\left|\\prod_{i=1}^K(1-h_i \\lambda)\\right|$ pour le cycle optimal et comparez le à $\\lambda \\mapsto \\left|(1-h_{opt} \\lambda)\\right|^K$ où $h_{\\mathrm{opt}}$ est le pas optimal de la méthode $\\text{(GradDesc)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1898c0c-9223-4aca-93a6-601f0c3c5a65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa285b91-69ce-49b7-bbba-19ce2c7877bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce6536f1-0085-438f-84e2-fb8a79e3f836",
   "metadata": {},
   "source": [
    "**Question 10 :** Écrivez la fonction `GradDescK(A,b, x0, h, k,K) ` qui applique l'algorithme du gradient $K$-cyclique à $f$, en partant de `x0` avec un vecteur de pas de temps `h`$\\in\\mathbb{R}^K$ qui\n",
    "- s'arrête après $k$ itérations,\n",
    "- renvoie la suite des itérés $(x_k)_k$ générée par l'aglorithme $\\text{GradDesc $K$-cyc}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c92333-8a3b-4ae2-bbe7-3dc9614a9ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846f33b4-9841-4a41-834b-e5a40be6ac45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cd84653-95d5-4976-8c88-868d185bd601",
   "metadata": {},
   "source": [
    "**Question 11 :**  Pour une matrice symétrique définie positive $A \\in \\mathcal{M}_{10} (\\mathbb{R})$ et un vecteur $b \\in \\mathbb{R}^{10}$ générés aléatoirement, tracez les courbes d'erreur $(\\| x_k - x^* \\|)_k$ en fonction de $k \\in [0,100]$, où $(x_k)_k$ a été générée avec l'algorithme $\\text{(GradDesc K-cyc)}$, pour différent $K$. Comparez les avec la courbe d'erreur obtenue avec la méthode du gradient classique $\\text{(GradDesc)}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90edc4a9-1425-4a3d-910b-ade3639d6f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ec8b92-c5b4-4645-8254-804679b9d531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb64129a-0b31-4c3f-a13a-a8fea9eda279",
   "metadata": {},
   "source": [
    "**Question 12 :** Que remarquez vous lorsqu'on augmente la valeur de $K$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99996a18-6aa8-43ba-ab99-ff124644be2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a714f3-e4a9-4adf-b611-d715d1106d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c81507d-309f-4cfc-a675-f6f100f1c9c7",
   "metadata": {},
   "source": [
    "## IV. Formulation par recurrence et cyclique infini\n",
    "\n",
    "\n",
    "Les polynômes de Tchebychev, utilisés pour trouver le cycle optimal à $K$ fixé dans la section précédente, sont une base de polynômes qui interviennent dans plusieurs algorithmes de l'analyse numérique (interpolations, intégration numériques, algorithmes d'orthogonalisation, résolution d'équation aux dérivées partielles) grace à leur proprietés. Ils peuvent être définis par récurrence par la suite\n",
    "$$\n",
    "    \\begin{cases}\n",
    "    T_0(x) = 1\\\\\n",
    "    T_1(x) = x\\\\\n",
    "    T_{k+1}(x) = 2xT_k(x) - T_{k-1}(x).\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "En utilisant cette suite, on peut définir l'algorithme de \"cycle infini\" suivant :\n",
    "\n",
    "\n",
    "$$\n",
    "    \\begin{cases}\n",
    "        x_0 \\in \\mathbb{R}^n \\\\\n",
    "        x_1 = x_0 - \\frac{2}{L+\\mu}A x_0\\\\\n",
    "        x_{k+1} = x_k + \\frac{4}{L-\\mu} \\frac{\\alpha_k}{\\alpha_{k+1}}  (b-Ax_k) + \\frac{\\alpha_{k-1}}{\\alpha_{k+1}}(x_k-x_{k-1}).\n",
    "    \\end{cases}\\tag{GradDesc $\\infty$-cyc}\n",
    "$$\n",
    "\n",
    "où la suite $(\\alpha_k)_k$ est définie par récurrence par \n",
    "$$\n",
    "    \\begin{cases}\n",
    "        \\alpha_0 = 1 \\\\\n",
    "        \\alpha_1 = \\frac{L+\\mu}{L-\\mu}\\\\\n",
    "        \\alpha_{k+1} = 2\\frac{L+\\mu}{L-\\mu} \\alpha_k - \\alpha_{k-1}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8ba5ed-7f4f-48fe-903b-d68bb88f9c4a",
   "metadata": {},
   "source": [
    "**Question 13 :** Écrivez la fonction `GradDescInf(A,b, x0, k) ` qui applique l'algorithme du gradient cycle infini à $f$, en partant de `x0` qui\n",
    "- s'arrête après $k$ itérations,\n",
    "- renvoie la suite des itérés $(x_k)_k$ générée par l'aglorithme $\\text{GradDesc $\\infty$-cyc}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c975527-b729-4491-bc93-210d344062a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf3c276-f959-4c8f-b83c-a0b5950c75ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7935dbb9-1e45-4709-8ad3-0d48602c1c5d",
   "metadata": {},
   "source": [
    "**Question 14 :** Pour une matrice symétrique définie positive $A \\in \\mathcal{M}_{10} (\\mathbb{R})$ et un vecteur $b \\in \\mathbb{R}^{10}$ générés aléatoirement, tracez la courbe d'erreur $(\\| x_k - x^* \\|)_k$ en fonction de $k \\in [0,100]$, où $(x_k)_k$ a été générée avec l'algorithme $\\text{(GradDesc $\\infty$-cyc)}$. Comparez la avec les courbes d'erreur obtenues avec les méthodes $\\text{(GradDesc)}$, $\\text{(GradDesc K-cyc)}$, pour différents $K$. Que constatez vous ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37ffe20-5296-40ca-a254-1090599161e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6429292f-5175-4e70-80ea-816f51e9e400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f47baeaa-fd24-439e-915d-b6b1b9c46340",
   "metadata": {},
   "source": [
    "## V. Visualisation en dimension 2\n",
    "\n",
    "Dans cette dernière section, on va chercher à visualiser les itérés des diffèrentes méthodes implémentées précédement sur les lignes de niveau d'une fonction quadratique en dimension 2.\n",
    "\n",
    "On définit la fonction $f \\, : \\, \\mathbb{R}^2 \\rightarrow \\mathbb{R}$ par\n",
    "$$\n",
    "f(x) = \\frac12 \\langle Ax, x \\rangle -\\langle b, x \\rangle ,\n",
    "$$\n",
    "avec \n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "9 & 7 \\\\\n",
    "7 & 9 \n",
    "\\end{pmatrix} \\quad \\text{ et }  \\quad\n",
    "b = \\begin{pmatrix}\n",
    "1\\\\\n",
    "-6\n",
    "\\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd1546a-f635-44d9-9eab-90b17b7462b0",
   "metadata": {},
   "source": [
    "**Question 15 :** Sur le même graphe, tracez les itérations des suites générées par $\\text{(GradDesc)}$, $\\text{(GradDesc 2-cyc)}$, $\\text{(GradDesc 5-cyc)}$ et $\\text{(GradDesc $\\infty$-cyc)}$ partant de $x_0 = (-5, -1)$ sur le graphe des lignes de niveaux de la fonction $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61075fdf-7779-4ee5-b5c5-55d7fe25b717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde26691-7f5c-4943-856f-b78ebed8b169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
